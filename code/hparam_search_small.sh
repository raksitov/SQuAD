set -x

OPT='adam'
CTX=300
QL=30
EMB=42
NE=25
B=170
LR=0.003
HS=100
NL=1
MS=75
ML=2
GN=10006.
D=0.2
CELL='lstm'
AL=15
BIDAF=false
SHARE=true
STATES=false
COMB='max'
EVERY=`expr 8600 / $B / 2`
python main.py \
  --mode=train \
  --use_rnn_for_ends=false \
  --reuse_question_states=$STATES \
  --share_encoder=$SHARE \
  --use_bidaf=$BIDAF \
  --h_model_size=$MS \
  --h_model_layers=$ML \
  --h_optimizer=$OPT \
  --num_epochs=$NE \
  --h_learning_rate=$LR \
  --h_context_len=$CTX \
  --h_question_len=$QL \
  --h_embedding_size=$EMB \
  --h_batch_size=$B \
  --h_hidden_size=$HS \
  --h_dropout=$D \
  --h_cell_type=$CELL \
  --h_num_layers=$NL \
  --h_max_gradient_norm=$GN \
  --h_answer_len=$AL \
  --h_combiner=$COMB \
  --eval_every=$EVERY \
  --save_every=$EVERY \
  --data_dir=`readlink -f ../small/` \
  --experiments_results=`readlink -f ../data/`"/experiments_results_small.json"
